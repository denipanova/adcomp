xlab("Dimension 1") +
ylab("Dimension 2") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
# saving
cairo_pdf("dataPlot.pdf")
print(dataPlot)
dev.off()
}
return(dataset)
}
banana<-genXOR(noObs=100,seed=1111, savePlot = FALSE)
View(banana)
plot(banana)
banana<-genXOR(noObs=100,seed=1111, savePlot = TRUE)
getwd()
kNN <- function(features, labels,
k = 1, p = 2){
# test the inputs
if (!require("assertthat")) install.packages("assertthat");
library(assertthat);
if (!require("plyr")) install.packages("plyr");
library(plyr);
not_empty(features);
not_empty(labels);
are_equal(nrow(features),length(labels));
#check that labels are integers
# is.numeric(feaures)
is.count(k);
assert_that(p %in% c(1, 2, Inf))
# Compute the distance between each point and all others
noObs <- nrow(features)
#distance matrix
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order) #1-apply to rows
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
predictedClasses<- rep(NA,noObs)
for (obs in 1:noObs) {
freq <- plyr::count(labels[neighbors[1:k,obs]])
maxfreq<- max(freq$freq)
lab<- freq[maxfreq==freq$freq,"x"]
predictedClasses[obs]<-lab[sample(length(lab),1)] #sample(5,1)
#take at rondom one of the biggest mode
prob[obs]<- (maxfreq/k)*100
}
#errorCount <- table(predictedClasses, labels)
#accuracy <- mean(predictedClasses == labels)
return(list(predLabels = predictedClasses,
prob = prob))
}
names(banana)
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = x1, y = x2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
#theme_bw() +
#theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
plot2dClasses(banana)
predict_5nn<-kNN(banana[,1:2],banana[,3],k=5,p=2)
pred<-cbind(banana,predict_5nn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
require(class)
x <- mixture.example$x
View(x)
g <- mixture.example$y
head(g)
xnew <- mixture.example$xnew
head(xnew)
mod15 <- knn(x, xnew, g, k=15, prob=TRUE)
prob <- attr(mod15, "prob")
head(prob)
px1 <- mixture.example$px1
head(p1)
head(px1)
summary(g\)
summary(g)
px1 <- mixture.example$px1
px2 <- mixture.example$px2
head(mixture.example)
str(mixture.example)
prob15 <- matrix(prob, length(px1), length(px2))
head(prob15)
View(prob15)
View(prob15)
kNN <- function(features, labels, train_set = NULL, k = 1, p = 2, type='train'){
# test the inputs
if (!require("assertthat")) install.packages("assertthat");
library(assertthat);
if (!require("plyr")) install.packages("plyr");
library(plyr);
not_empty(features);
not_empty(labels);
are_equal(nrow(features),length(labels));
#check that labels are integers
is.numeric(features)
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
if (type == "predict") {
assert_that(not_empty(train_set) &
ncol(train_set) == ncol(features) &
nrow(train_set) == length(labels))
}
# Compute the distance between each point and all others
noObs <- nrow(features)
noVars <- ncol(features)
#distance matrix
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(features -  probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
notrain_set <- nrow(train_set)
distMatrix <- matrix(NA, noObs, notrain_set)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = notrain_set, ncol = ncol(features), byrow = TRUE)
# computing distances between the probe and exemplars in the train_set
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(train_set - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(train_set - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order) #1-apply to rows
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
predictedClasses<- rep(NA,noObs)
for (obs in 1:noObs) {
freq <- plyr::count(labels[neighbors[1:k,obs]])
maxfreq<- max(freq$freq)
lab<- freq[maxfreq==freq$freq,"x"]
predictedClasses[obs]<-lab[sample(length(lab),1)] #sample(5,1)
#take at rondom one of the biggest mode
prob[obs]<- (maxfreq/k)*100
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount))
}
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
# save the dataset in a CSV format
if (saveData) {
write.csv(dataset, file="dataset.csv", row.names = FALSE)
}
# save the plot
if (savePlot) {
# generating the plot
dataPlot <-
ggplot(data = dataset,
aes(x = x1, y = x2, color = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("Dimension 1") +
ylab("Dimension 2") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
# saving
cairo_pdf("dataPlot.pdf")
print(dataPlot)
dev.off()
}
return(dataset)
}
dataset<-genXOR(noObs=100,seed=1111, savePlot = FALSE)
predict_5nn<-kNN(dataset[,1:2],dataset[,3],train_set = NULL, k = 5, p = 2, type='train')
pred<-cbind(dataset,predict_5nn)
write.csv(pred,"predictions.csv")
getwd()
setwd("BGSE/semester2/adcomp/adcomp/")
pred<-cbind(dataset,predict_5nn$predLabels, predict_5nn$prob)
write.csv(pred,"predictions.csv")
names(pred)
colnames(pred)<-c("x1","x2","y","predLabels","prob")
names(pred)
x1 <- seq(min(dataset$x1), max(dataset$x1), by=0.2)
x2 <- seq(min(dataset$x2), max(dataset$x2), by=0.2)
#create grid with x1 and x2
sample <- expand.grid(x1=x1, x2=x2)
# let's use the kNN function to predict
#the sample labels
predSample <- kNN(features=sample, labels=dataset[,3], train_set=dataset[,1:2], k=1, p=2, type="predict")
predSampleLab <- predSample$predLabels
prob <- matrix(predSampleLab, length(x1), length(x2))
plot<-ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=3, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
plot
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=3, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
geom_tile(aes(fill = prob)) + stat_contour() +
#stat_contour(bins=1) +
geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=3, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data = dataset,
aes(x = x1, y = x2, color = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("Dimension 1") +
ylab("Dimension 2") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
ggplot(data=grid, aes(x=x1, y=x2, z=predGridClasses)) +
stat_contour(bins=1) +
geom_point(aes(x=x1, y=x2, colour=as.factor(predGridClasses))) +
geom_point(data=dataset, size=3, aes(x=x1, y=x2, z=y)) +
theme_bw()
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=3, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4,shape = 4, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4,shape = 4, aes(x=x1, y=x2, z=y, shape=as.factor(dataset$y))+
scale_shape(solid = FALSE))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4, aes(x=x1, y=x2, z=y, shape=factor(dataset$y))+
scale_shape(solid = FALSE))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
aes(shape = factor(dataset$y)) +
scale_shape(solid = FALSE)+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
aes(shape = factor(dataset$y)) +
scale_shape(solid = FALSE)+
geom_point(data=dataset, size=4, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4, shape=4, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab))) +
geom_point(data=dataset, size=4, shape=5, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab, alpha(0.2)))) +
geom_point(data=dataset, size=4, shape=5, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.2)) +
geom_point(data=dataset, size=4, shape=5, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y)))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.2)) +
geom_point(data=dataset, size=4, shape=5, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
#geom_bar()+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
scale_colour_continuous(guide = FALSE) +
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
#scale_fill_brewer(palette="Paired")+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
scale_colour_continuous(guide = FALSE) +
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
#geom_tile(aes(fill = prob)) + stat_contour() +
stat_contour(bins=1) +
scale_alpha_continuous(guide=FALSE)+
#scale_colour_continuous(guide = FALSE) +
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
plot<-ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
stat_contour(bins=1) +
scale_alpha_continuous(guide=FALSE)+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggsave(plot,"plot.pdf")
plotFunc<-function(){
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
stat_contour(bins=1) +
scale_alpha_continuous(guide=FALSE)+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
}
ggsave(plotFunc,"plot.pdf")
ggsave(plotFunc(),"plot.pdf")
plotFunc<-function() {
ggplot(data=sample, aes(x=x1, y=x2, z=predSampleLab)) +
stat_contour(bins=1) +
scale_alpha_continuous(guide=FALSE)+
geom_point(aes(x=x1, y=x2, colour=as.factor(predSampleLab), alpha=0.3)) +
geom_point(data=dataset, size=4, shape=7, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y), alpha=1))+
labs(colour="LABELS")+
ggtitle("DATA AND DECISION BOUNDARIES")
ggsave("plot.pdf")
}
plotFunc()
train<-read.csv("MNIST_training.csv", header = FALSE)
source("kNNNoDist.R")
source("DistanceMatrix.R")
library("class")
#Take random small sample in order to validate
s_indeces<-sample.int(6000, size = 1000)
s<- train[s_indeces,]
#split the data into two
tr<-sample.int(1000, size=300)
val<-setdiff(1:1000,tr)
training_set<- s[tr,]
training_x<- training_set[,-1]
training_y<- training_set[,1]
validation_set<- s[val,]
validation_x<-validation_set[,-1]
validation_y<-validation_set[,1]
kMeasures<- c(1,3,5,7,9,11,15,17,23,25,35,45,55,83,101,151 )
distMeasure<-c(1,2,3)
accuracyMatrix<-matrix(NA,length(kMeasures), length(distMeasure))
for (j in 1:length(distMeasure)){
distMatrix<-DistanceMatrix(validation_x,training_x, p=distMeasure[j])
for (i in 1:length(kMeasures)){
prediction <- kNNNoDist(distMatrix, training_y, k=kMeasures[i])
accuracyMatrix[i,j] <- mean(prediction$predLabels == validation_y)
}
}
optimal<-which.max(accuracy) #takes teh index of the best one
View(accuracyMatrix)
View(accuracyMatrix)
max.col(accuracyMatrix)
